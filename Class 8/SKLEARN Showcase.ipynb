{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import RandomizedPCA , PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'data', 'target_names', 'DESCR', 'target']\n",
      "Optical Recognition of Handwritten Digits Data Set\n",
      "===================================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "print digits.keys()\n",
    "print digits['DESCR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all pixels are useful in identifying the digit. Only few of the pixels has high correlation with the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.\n",
      "    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.\n",
      "    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.\n",
      "   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.\n",
      "    0.   0.   6.  13.  10.   0.   0.   0.]\n",
      " [  0.   0.   0.  12.  13.   5.   0.   0.   0.   0.   0.  11.  16.   9.\n",
      "    0.   0.   0.   0.   3.  15.  16.   6.   0.   0.   0.   7.  15.  16.\n",
      "   16.   2.   0.   0.   0.   0.   1.  16.  16.   3.   0.   0.   0.   0.\n",
      "    1.  16.  16.   6.   0.   0.   0.   0.   1.  16.  16.   6.   0.   0.\n",
      "    0.   0.   0.  11.  16.  10.   0.   0.]]\n",
      "(1797L, 64L)\n"
     ]
    }
   ],
   "source": [
    "features = digits['data']\n",
    "labels = digits['target']\n",
    "\n",
    "print features[:2]\n",
    "print features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   1.,   9.,  15.,  11.,   0.,   0.],\n",
       "       [  0.,   0.,  11.,  16.,   8.,  14.,   6.,   0.],\n",
       "       [  0.,   2.,  16.,  10.,   0.,   9.,   9.,   0.],\n",
       "       [  0.,   1.,  16.,   4.,   0.,   8.,   8.,   0.],\n",
       "       [  0.,   4.,  16.,   4.,   0.,   8.,   8.,   0.],\n",
       "       [  0.,   1.,  16.,   5.,   1.,  11.,   3.,   0.],\n",
       "       [  0.,   0.,  12.,  12.,  10.,  10.,   0.,   0.],\n",
       "       [  0.,   0.,   1.,  10.,  13.,   3.,   0.,   0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[10].reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb0a0f98>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb113320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAAD7CAYAAAC2TgIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb113358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.gray()\n",
    "plt.matshow(features[0].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing the number of features to the useful ones\n",
    "\n",
    "Let us use the Principal component analysis and determine the first 30 pixels that has high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=64, whiten=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=64)\n",
    "pca.fit(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14890594,  0.13618771,  0.11794594,  0.08409979,  0.05782415,\n",
       "        0.0491691 ,  0.04315987,  0.03661373,  0.03353248,  0.03078806])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797L, 64L)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_features = pca.transform(features)\n",
    "main_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Let us split our dataset into training and test sets using the cross_validation module in SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features,test_features,train_labels,test_labels = train_test_split(main_features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347L, 64L)\n",
      "(450L, 64L)\n",
      "(1347L,)\n",
      "(450L,)\n"
     ]
    }
   ],
   "source": [
    "print train_features.shape\n",
    "print test_features.shape\n",
    "print train_labels.shape\n",
    "print test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us use a KNN classifier first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98666666666666669"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(test_features,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8288888888888889"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_features,train_labels)\n",
    "dt.score(test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92000000000000004"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1 = RandomForestClassifier()\n",
    "dt1.fit(train_features,train_labels)\n",
    "dt1.score(test_features,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Support Vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(train_features,train_labels)\n",
    "svc.score(test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Using ADA Boost search\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92666666666666664"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = GradientBoostingClassifier()\n",
    "ada.fit(train_features,train_labels)\n",
    "ada.score(test_features,test_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
